{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "stopwords = set(ENGLISH_STOP_WORDS)\n",
    "stopwords.add(\"tomorrow\")\n",
    "stopwords.add(\"day\")\n",
    "stopwords.add(\"http\")\n",
    "stopwords.add(\"3rd\")\n",
    "stopwords.add(\"u002c\")\n",
    "stopwords.add(\"time\")\n",
    "stopwords.add(\"1st\")\n",
    "stopwords.add(\"today\")\n",
    "stopwords.add(\"make\")\n",
    "stopwords.add(\"think\")\n",
    "stopwords.add(\"u2019m\")\n",
    "stopwords.add(\"saturday\")\n",
    "stopwords.add(\"tuesday\")\n",
    "stopwords.add(\"wednesday\")\n",
    "stopwords.add(\"people\")\n",
    "stopwords.add(\"did\")\n",
    "stopwords.add(\"will\")\n",
    "stopwords.add(\"said\")\n",
    "stopwords.add(\"say\")\n",
    "stopwords.add(\"says\")\n",
    "stopwords.add(\"it\")\n",
    "stopwords.add(\"they\")\n",
    "stopwords.add(\"are\")\n",
    "stopwords.add(\"that\")\n",
    "stopwords.add(\"saying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ REMOVE SPECIAL CHARS AND STOPWORDS from train\n",
    "trainData = pd.read_csv('train2017.tsv', sep=\"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "stemmed = []\n",
    "removed = []  # cleaned tweets\n",
    "tokens = []\n",
    "similar_words = []\n",
    "vect = []\n",
    "tokenized_tweet = []\n",
    "\n",
    "for i in range(0, trainData.shape[0]):\n",
    "\tremoved.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', trainData.loc[i][3]))\n",
    "\n",
    "\tword_tokens = word_tokenize(removed[i])\n",
    "\tfiltered_sentence = []\n",
    "\tfor w in word_tokens: \n",
    "\t\tif w.lower() not in stopwords: \n",
    "\t\t\tfiltered_sentence.append(w.lower())\n",
    "\n",
    "\t\t\t# STEM\n",
    "\t\t\tporter = PorterStemmer()\n",
    "\t\t\tstemmed.extend([porter.stem(w.lower())]) \n",
    "\ttokens.append(filtered_sentence)\n",
    "\tremoved[i] = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ REMOVE SPECIAL CHARS AND STOPWORDS from test\n",
    "testData = pd.read_csv('test2017.tsv', sep=\"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "stemmed2 = []\n",
    "removed2 = []  # cleaned tweets\n",
    "tokens2 = []\n",
    "similar_words2 = []\n",
    "vect2 = []\n",
    "tokenized_tweet2 = []\n",
    "\n",
    "for i in range(0, testData.shape[0]):\n",
    "\tremoved2.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', testData.loc[i][3]))\n",
    "\n",
    "\tword_tokens2 = word_tokenize(removed2[i])\n",
    "\tfiltered_sentence2 = []\n",
    "\tfor w in word_tokens2: \n",
    "\t\tif w.lower() not in stopwords: \n",
    "\t\t\tfiltered_sentence2.append(w.lower())\n",
    "\n",
    "\t\t\t# STEM\n",
    "\t\t\tporter2 = PorterStemmer()\n",
    "\t\t\tstemmed2.extend([porter2.stem(w.lower())]) \n",
    "\ttokens2.append(filtered_sentence2)\n",
    "\tremoved2[i] = ' '.join(filtered_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE WORD EMBEDDINGS\n",
    "\n",
    "# ------------------ w2v train ------------------\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokens,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(tokens, total_examples= len(removed), epochs=20)\n",
    "\n",
    "model_w2v.save('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD WORD EMBEDDINGS\n",
    "\n",
    "new_model = gensim.models.Word2Vec.load('embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFFIN LEXICA\n",
    "\n",
    "affin = pd.read_csv('affin.txt', sep=\"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "affin_lexica = []\n",
    "for i in range(0, affin.shape[0]):\n",
    "\tvals = {}\n",
    "\tvals['word'] = affin.loc[i][0]\n",
    "\tvals['vector'] = affin.loc[i][1]\n",
    "\taffin_lexica.append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athina/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TWEET VECTORS\n",
    "\n",
    "# ------------------ w2v train ------------------\n",
    "\n",
    "vec = []\n",
    "vec_tweet = []   # list with the vectors of each tweet\n",
    "\n",
    "for i in range(0, trainData.shape[0]):\n",
    "\tvec = np.zeros(201)  # pos 201 for lexica\n",
    "\twcount = len(removed[i].split())\n",
    "\tfor word in removed[i].split():\n",
    "\t\tfor j in range(0,200):\n",
    "\t\t\tif word in new_model.wv.vocab:\n",
    "\t\t\t\tvec[j] += (new_model[word][j] / wcount)\n",
    "\t\tif word in affin_lexica:\n",
    "\t\t\tvec[200] = affin_lexica[word]\n",
    "\tvec_tweet.append(vec)\n",
    "\n",
    "# print (vec_tweet[0])\n",
    "\n",
    "# EXPORT TWEET VECTORS TO FILE\n",
    "tv_output = open(\"tweetvectors.pkl\", \"wb\")\n",
    "pickle.dump(vec_tweet, tv_output)\n",
    "tv_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TWEET VECTORS for train\n",
    "\n",
    "tv_file = open(\"tweetvectors.pkl\", \"rb\")\n",
    "vec_tweet = pickle.load(tv_file)\n",
    "tv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athina/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TWEET VECTORS\n",
    "\n",
    "# ------------------ w2v test ------------------\n",
    "\n",
    "vec2 = []\n",
    "vec_tweet_test = []    # list with the vectors of each tweet\n",
    "\n",
    "for i in range(0, testData.shape[0]):\n",
    "\tvec2 = np.zeros(201)\n",
    "\twcount = len(removed2[i].split())\n",
    "\tfor word in removed2[i].split():\n",
    "\t\tfor j in range(0,200):\n",
    "\t\t\tif word in new_model.wv.vocab:\n",
    "\t\t\t\tvec2[j] += (new_model[word][j] / wcount)\n",
    "\t\tif word in affin_lexica:\n",
    "\t\t\tvec2[200] = affin_lexica[word]\n",
    "\tvec_tweet_test.append(vec2)\n",
    "\n",
    "# EXPORT TWEET VECTORS TO FILE\n",
    "tv2_output = open(\"tweetvectorstest.pkl\", \"wb\")\n",
    "pickle.dump(vec_tweet_test, tv2_output)\n",
    "tv2_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TWEET VECTORS for test\n",
    "\n",
    "tv2_file = open(\"tweetvectorstest.pkl\", \"rb\")\n",
    "vec_tweet_test = pickle.load(tv2_file)\n",
    "tv2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG OF WORDS\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "bow_xtrain = bow_vectorizer.fit_transform(removed)\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "bow_vectorizer2 = CountVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "bow_xtrain2 = bow_vectorizer2.fit_transform(removed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT BAG TO FILE\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "bow_output = open(\"bagofwords.pkl\", \"wb\")\n",
    "pickle.dump(bow_xtrain, bow_output)\n",
    "bow_output.close()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "bow_output2 = open(\"bagofwords_test.pkl\", \"wb\")\n",
    "pickle.dump(bow_xtrain2, bow_output2)\n",
    "bow_output2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BAG\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "bow_file = open(\"bagofwords.pkl\", \"rb\")\n",
    "bag = pickle.load(bow_file)\n",
    "bow_file.close()\n",
    "\n",
    "bow_xtrain = bag\n",
    "features = bow_vectorizer.get_feature_names()\n",
    "bag = bag.toarray()\n",
    "bow_df = pd.DataFrame(bag, columns = features)\n",
    "\n",
    "bow_xtrain = bow_xtrain.toarray()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "bow_file2 = open(\"bagofwords_test.pkl\", \"rb\")\n",
    "bag2 = pickle.load(bow_file2)\n",
    "bow_file2.close()\n",
    "\n",
    "bow_xtrain_test = bag2\n",
    "features2 = bow_vectorizer2.get_feature_names()\n",
    "bag2 = bag2.toarray()\n",
    "bow_df2 = pd.DataFrame(bag2, columns = features2)\n",
    "\n",
    "bow_xtrain_test = bow_xtrain_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "s = []\n",
    "for i in range(0,trainData.shape[0]):\n",
    "\ts.extend([removed[i]])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(s)\n",
    "tfidf = tfidf.toarray()\n",
    "features = tfidf_vectorizer.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(np.round(tfidf, 3), columns = features)\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "s2 = []\n",
    "for i in range(0,testData.shape[0]):\n",
    "\ts2.extend([removed2[i]])\n",
    "\n",
    "tfidf_vectorizer2 = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000, stop_words='english') \n",
    "tfidf2 = tfidf_vectorizer2.fit_transform(s2)\n",
    "tfidf2 = tfidf2.toarray()\n",
    "features2 = tfidf_vectorizer2.get_feature_names()\n",
    "tfidf_df2 = pd.DataFrame(np.round(tfidf2, 3), columns = features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT TF_IDF TO FILE\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "tfidf_output = open(\"tfidf.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_df, tfidf_output)\n",
    "tfidf_output.close()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "tfidf_output2 = open(\"tfidf_test.pkl\", \"wb\")\n",
    "pickle.dump(tfidf_df2, tfidf_output2)\n",
    "tfidf_output2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TF_IDF\n",
    "\n",
    "# ------------------ train ------------------\n",
    "\n",
    "tfidf_file = open(\"tfidf.pkl\", \"rb\")\n",
    "tf = pickle.load(tfidf_file)\n",
    "tfidf_file.close()\n",
    "\n",
    "# ------------------ test ------------------\n",
    "\n",
    "tfidf_file2 = open(\"tfidf_test.pkl\", \"rb\")\n",
    "tf_test = pickle.load(tfidf_file2)\n",
    "tfidf_file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ LABELS\n",
    "\n",
    "labels = []\n",
    "for i in range(0, trainData.shape[0]):\n",
    "\tlabels.append(trainData.loc[i][2])\n",
    "\n",
    "testLabels = pd.read_csv('SemEval2017_task4_subtaskA_test_english_gold.txt', sep=\"\\t\", encoding = \"utf-8\")\n",
    "labels_test = []\n",
    "for i in range(0, testLabels.shape[0]):\n",
    "\tlabels_test.append(testLabels.loc[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN W2V ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_tweet, labels, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN W2V FOR TEST\n",
    "\n",
    "X_train = vec_tweet\n",
    "y_train = labels\n",
    "X_test = vec_tweet_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.512822600341936\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47032483920866236\n"
     ]
    }
   ],
   "source": [
    "# Repeat for KNN with K=5:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM W2V ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vec_tweet, labels, random_state=42, test_size=0.2) #input for this method is any array of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM W2V FOR TEST\n",
    "\n",
    "X_train = vec_tweet\n",
    "y_train = labels\n",
    "X_test = vec_tweet_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5171375071236669\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test) #predict on the validation set\n",
    "print (f1_score(y_test, prediction, average='micro')) #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN BAG-OF-WORDS ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_xtrain, labels, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN BAG-OF-WORDS FOR TEST\n",
    "\n",
    "X_train = bow_xtrain\n",
    "y_train = labels\n",
    "X_test = bow_xtrain_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.393552063828055\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3998208906618904\n"
     ]
    }
   ],
   "source": [
    "# Repeat for KNN with K=5:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "# print (y_pred)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM BAG-OF-WORDS ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_xtrain, labels, random_state=42, test_size=0.2) #input for this method is any array of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM BAG-OF-WORDS FOR TEST\n",
    "\n",
    "X_train = bow_xtrain\n",
    "y_train = labels\n",
    "X_test = bow_xtrain_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39607587722869003\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test) #predict on the validation set\n",
    "print (f1_score(y_test, prediction, average='micro')) #evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN TF-IDF ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf, labels, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN TF-IDF FOR TEST\n",
    "\n",
    "X_train = tf\n",
    "y_train = labels\n",
    "X_test = tf_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3980297972807946\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3980297972807946\n"
     ]
    }
   ],
   "source": [
    "# Repeat for KNN with K=5:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "# print (y_pred)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TF-IDF ONLY FOR TRAIN (train_test_split)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf, labels, random_state=42, test_size=0.2) #input for this method is any array of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM TF-IDF FOR TEST\n",
    "\n",
    "X_train = tf\n",
    "y_train = labels\n",
    "X_test = tf_test\n",
    "y_test = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3947732638606204\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "prediction = svc.predict(X_test) #predict on the validation set\n",
    "print (f1_score(y_test, prediction, average='micro')) #evaluate on the validation set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
